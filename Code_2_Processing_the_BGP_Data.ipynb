{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "from html.parser import HTMLParser\n",
    "import pybgpstream\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, collect_list\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import sqlite3\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the database class \n",
    "class DBConnection:\n",
    "    def __init__(self, dbname = 'data.db'):\n",
    "        self.db_con = sqlite3.connect(dbname)\n",
    "        self.db_cur = self.db_con.cursor()\n",
    "    \n",
    "    def create_table(self):\n",
    "        self.db_cur.execute('''create table if not exists au_systems (\n",
    "            asn integer PRIMARY KEY,\n",
    "            organization text,\n",
    "            country text\n",
    "        )''')\n",
    "\n",
    "    def insert(self, asn, org_info, country):\n",
    "        self.db_cur.execute(f\"insert into au_systems values ('{asn}', '{org_info}', '{country}')\")\n",
    "\n",
    "    def insert(self, asn, org_info, country):\n",
    "        try:\n",
    "            self.db_cur.execute(\"INSERT OR IGNORE INTO au_systems VALUES (?, ?, ?)\", (asn, org_info, country))\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(\"Database locked. Retrying in 1 second...\")\n",
    "            time.sleep(1) \n",
    "            # Retry the insert operation\n",
    "            self.insert(asn, org_info, country)  \n",
    "\n",
    "    def find(self, column, value):\n",
    "        self.db_cur.execute(f\"select * from au_systems where {column}='{value}'\")\n",
    "        return self.db_cur.fetchall()\n",
    "\n",
    "    def find_one(self, column, value):\n",
    "        self.db_cur.execute(f\"select * from au_systems where {column}='{value}'\")\n",
    "        return self.db_cur.fetchone()\n",
    "    \n",
    "    def find_all(self):\n",
    "        self.db_cur.execute('select * from au_systems')\n",
    "        return self.db_cur.fetchall()\n",
    "    \n",
    "    def commit(self):\n",
    "        self.db_con.commit()\n",
    "\n",
    "    def close(self):\n",
    "        self.db_con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the database for ASNs \n",
    "\n",
    "Only run the cell block below if you have not yet created this database before or if you want to update the exicting one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HTML Parser Class\n",
    "# class HTMLFilter(HTMLParser):\n",
    "#     text = \"\"\n",
    "#     def handle_data(self, data):\n",
    "#         self.text += data\n",
    "        \n",
    "# html_filter = HTMLFilter()\n",
    "\n",
    "# # Download content from the resource\n",
    "# url = \"https://www.cidr-report.org/as2.0/autnums.html\"\n",
    "# r = requests.get(url, allow_redirects = True)\n",
    "# html_filter.feed(r.content.decode(\"utf-8\"))\n",
    "\n",
    "# db = DBConnection()\n",
    "# db.create_table()\n",
    "\n",
    "# # Populate the database\n",
    "# lst = html_filter.text.splitlines( )\n",
    "# for i in range(14,len(lst)-8):\n",
    "#     line = lst[i]\n",
    "#     asn, org_info, country_code = int(line[2:8].strip()), line[8:-4].strip(), line[-2:]\n",
    "#     db.insert(asn, org_info, country_code)\n",
    "\n",
    "# db.commit()\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Process the data and translate the Country Codes for the ASNs\n",
    "def preprocess(df):\n",
    "    # Convert Path column to tuples\n",
    "    def path_to_tuple(path):\n",
    "        return tuple(path)\n",
    "\n",
    "    path_to_tuple_udf = udf(path_to_tuple, ArrayType(StringType()))\n",
    "\n",
    "    # Define the list of invalid country codes\n",
    "    invalid_country_codes = ['EU', 'ZZ']\n",
    "    db = DBConnection()\n",
    "    zz_eu_count = 0\n",
    "    unknown_count = 0\n",
    "\n",
    "    def process_path(path):\n",
    "        nonlocal zz_eu_count, unknown_count\n",
    "        db = DBConnection()  \n",
    "        country_path = []\n",
    "        count = 0\n",
    "        count2 = 0\n",
    "        for asn in path:\n",
    "            result = db.find('asn', asn)\n",
    "            if result:\n",
    "                if result[0][2] not in invalid_country_codes:\n",
    "                    country_path.append(result[0][2]) \n",
    "                else:\n",
    "                    count2 += 1\n",
    "            else:\n",
    "                count += 1\n",
    "        db.close() \n",
    "        zz_eu_count += count2\n",
    "        unknown_count += count\n",
    "        return country_path\n",
    "\n",
    "    process_path_udf = udf(process_path, ArrayType(StringType()))\n",
    "\n",
    "    # Apply the UDF to convert Path column to tuples\n",
    "    df = df.withColumn(\"Path\", path_to_tuple_udf(df[\"Path\"]))\n",
    "\n",
    "    # Apply the UDF to process paths and translate ASNs to country codes\n",
    "    df = df.withColumn(\"processed_path\", process_path_udf(df[\"Path\"]))\n",
    "\n",
    "    # Separate country_path, count, and count2\n",
    "    df = df.withColumn(\"country_path\", df[\"processed_path\"])\n",
    "\n",
    "    # Drop the processed_path column\n",
    "    df = df.drop(\"processed_path\")\n",
    "    print(f'{zz_eu_count} cases of asns found that are invalid (ZZ or EU)')\n",
    "    print(f'{unknown_count} cases of asns found that belong to unknown countries')\n",
    "    return df\n",
    "\n",
    "# Transform the data for more efficient storage\n",
    "def transformation3(df_update):\n",
    "    transformed_df = df_update.groupBy(\"Path\").agg(\n",
    "        count(\"Time\").alias(\"Frequency\"),\n",
    "        collect_list(\"country_path\").alias(\"country_paths\"), # Won't recomment this line for storage complexity\n",
    "        collect_list(\"Router Collector Name\").alias(\"Router_Collectors\") # Won't recomment this line for storage complexity\n",
    "    ).withColumn(\"path_length\", F.size(\"Path\"))\n",
    "\n",
    "    # Remove duplicate values from the \"Router_Collectors\" column\n",
    "    transformed_df = transformed_df.withColumn(\"Router_Collectors\", F.array_distinct(\"Router_Collectors\"))\n",
    "    transformed_df = transformed_df.withColumn(\"country_paths\", F.array_distinct(\"country_paths\"))\n",
    "\n",
    "    return transformed_df\n",
    "\n",
    "# Combine data of all days of belonging to the same year \n",
    "def combine_datasets_for_january(spark, year, day_range):\n",
    "\n",
    "    # Create an empty DataFrame to store combined data for January\n",
    "    combined_df = None\n",
    "\n",
    "    # Loop through each day in January (1st to 7th)\n",
    "    for day in range(1, day_range+1):\n",
    "        # Load the dataset for the specific day and year\n",
    "        file_path = f'BGP_data_{year}_j{day}'\n",
    "        df = spark.read.parquet(file_path)\n",
    "\n",
    "        # Combine the DataFrame with previous ones\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            combined_df = combined_df.union(df)\n",
    "    combined_df.write.format(\"parquet\").save(f'combined_{year}')\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Process the data\n",
    "def data_process2(spark, year):\n",
    "    \n",
    "    df = spark.read.parquet(f'combined_{year}')\n",
    "    df2 = preprocess(df)\n",
    "    df3 = transformation3(df2)\n",
    "    df3.write.format(\"parquet\").save(f'second_transformed_{year}')\n",
    "    return df3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------------+-------------------+\n",
      "|Peer AS|                Path|Router Collector Name|               Time|\n",
      "+-------+--------------------+---------------------+-------------------+\n",
      "|   8763| [8763, 6939, 24441]|                rrc12|2017-02-01 11:09:24|\n",
      "|   8763| [8763, 6939, 38235]|                rrc12|2017-02-01 11:09:24|\n",
      "|   8763| [8763, 6939, 38235]|                rrc12|2017-02-01 11:09:24|\n",
      "|   8763| [8763, 6939, 38235]|                rrc12|2017-02-01 11:09:24|\n",
      "|  36351|[36351, 1299, 132...|      route-views.isc|2017-02-01 11:09:26|\n",
      "|  36351|[36351, 1299, 132...|      route-views.isc|2017-02-01 11:09:26|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 24218, 9902]|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 9304, 553...|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 9304, 553...|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 9304, 553...|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 9304, 553...|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 59318, 13...|     route-views.linx|2017-02-01 11:09:27|\n",
      "|  58511|[58511, 59318, 13...|     route-views.linx|2017-02-01 11:09:27|\n",
      "+-------+--------------------+---------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22'), Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22'), Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22'), Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22'), Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22'), Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235, 38566], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22'), Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235, 132098], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22'), Row(Peer AS=53013, Path=[53013, 4809, 3549, 3356, 3491, 3491, 45796, 38235, 132098], Router Collector Name='route-views.saopaulo', Time='2017-02-07 02:56:22')]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Combiner\") \\\n",
    "            .config(\"spark.executor.memory\", \"2g\") \\\n",
    "            .config(\"spark.driver.memory\", \"2g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n",
    "# Call the function to combine datasets for January for each year\n",
    "combined_january_data = combine_datasets_for_january(spark, '2017', 7)\n",
    "\n",
    "# Show the combined DataFrame\n",
    "combined_january_data.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cases of asns found that are invalid (ZZ or EU)\n",
      "0 cases of asns found that belong to unknown countries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+-----------+\n",
      "|                Path|Frequency|       country_paths|   Router_Collectors|path_length|\n",
      "+--------------------+---------+--------------------+--------------------+-----------+\n",
      "|[10026, 1299, 483...|       39|[[JP, SE, CN, TH,...|[route-views3, ro...|          6|\n",
      "|[10026, 1299, 819...|        6|[[JP, SE, LV, LV,...|[route-views3, ro...|          5|\n",
      "|[10026, 15412, 99...|        8|  [[JP, GB, KH, KH]]|      [route-views3]|          4|\n",
      "|[10026, 174, 1299...|        3|[[JP, US, SE, SG,...|[route-views3, ro...|         18|\n",
      "|[10026, 174, 3491...|      113|[[JP, US, US, TH,...|[route-views3, ro...|          6|\n",
      "|[10026, 20804, 50...|        4|[[JP, PL, PL, PL,...|      [route-views3]|         12|\n",
      "|[10026, 24218, 13...|        3|      [[JP, MY, KH]]|[route-views.sydn...|          3|\n",
      "|[10026, 3257, 349...|       36|[[JP, US, US, TH,...|[route-views3, ro...|          6|\n",
      "|[10026, 3257, 349...|       95|[[JP, US, US, TH,...|[route-views.sydn...|          5|\n",
      "|[10026, 3356, 289...|       24|[[JP, US, RU, RU,...|[route-views3, ro...|          5|\n",
      "|[10026, 3356, 900...|        4|[[JP, US, GB, LV,...|[route-views.sydn...|          5|\n",
      "|[10026, 3491, 24441]|       32|      [[JP, US, KH]]|      [route-views3]|          3|\n",
      "|[10026, 3491, 457...|       40|[[JP, US, TH, KH,...|[route-views3, ro...|          5|\n",
      "|[10026, 3491, 553...|       14|  [[JP, US, KH, KH]]|[route-views3, ro...|          4|\n",
      "|[10026, 3491, 553...|        3|      [[JP, US, KH]]|[route-views3, ro...|          4|\n",
      "|[10026, 4637, 456...|       49|[[JP, HK, TH, VN,...|[route-views3, ro...|         12|\n",
      "|[10026, 7473, 242...|        5|  [[JP, SG, MY, KH]]|[route-views3, ro...|          4|\n",
      "|[10026, 7473, 755...|       10|[[JP, SG, VN, KH,...|[route-views.sydney]|          5|\n",
      "|[10026, 8220, 156...|        8|  [[JP, GB, ES, ES]]|      [route-views3]|          4|\n",
      "|[10026, 9304, 132...|      107|  [[JP, HK, TH, KH]]|[route-views3, ro...|          4|\n",
      "+--------------------+---------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GROOT\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = data_process2(spark, '2017_feb')\n",
    "df.show()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
