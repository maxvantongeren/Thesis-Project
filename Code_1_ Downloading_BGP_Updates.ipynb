{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "from html.parser import HTMLParser\n",
    "import pybgpstream\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, collect_list\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import sqlite3\n",
    "from pyspark.sql.functions import concat_ws\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the code in the cell below for turning off all warnings when downloading data from 2020 or later\n",
    "\n",
    "BGPStream has a bug where a lot of warnings get produced if the updates originate from the year 2020 or later. Leaving the warnings on can cause the kernel to crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/15 14:54:35 WARN Utils: Your hostname, BIGBOY resolves to a loopback address: 127.0.1.1; using 192.168.178.27 instead (on interface eno2)\n",
      "24/06/15 14:54:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/15 14:54:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "1718456227 HTTP ERROR: Failure when receiving data from the peer (56)\n",
      "1718458299 HTTP ERROR: Failure when receiving data from the peer (56)\n",
      "1718459823 HTTP ERROR: Failure when receiving data from the peer (56)\n",
      "1718461418 HTTP ERROR: Failure when receiving data from the peer (56)\n",
      "1718462990 HTTP ERROR: Failure when receiving data from the peer (56)\n",
      "24/06/15 17:14:08 WARN TaskSetManager: Stage 0 contains a task of very large size (11046 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/15 17:14:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "1718467449 HTTP ERROR: Timeout was reached (28)                                 \n",
      "2024-06-15 18:04:09 5264: bs_transport_file.c:39: ERROR: Could not open http://archive.routeviews.org/route-views.isc/bgpdata/2017.02/UPDATES/updates.20170204.1015.bz2 for reading\n",
      "2024-06-15 18:04:09 5264: bgpstream_transport.c:97: ERROR: Could not open resource (http://archive.routeviews.org/route-views.isc/bgpdata/2017.02/UPDATES/updates.20170204.1015.bz2)\n",
      "2024-06-15 18:04:09 5264: bgpstream_reader.c:169: WARNING: Could not open (http://archive.routeviews.org/route-views.isc/bgpdata/2017.02/UPDATES/updates.20170204.1015.bz2). Attempt 1 of 5\n",
      "1718468037 HTTP ERROR: Timeout was reached (28)\n",
      "24/06/15 19:50:44 WARN TaskSetManager: Stage 1 contains a task of very large size (11697 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/15 19:50:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "1718474988 HTTP ERROR: Transferred a partial file (18)                          \n",
      "1718475788 HTTP ERROR: Timeout was reached (28)\n",
      "1718475849 HTTP ERROR: Timeout was reached (28)\n",
      "1718481744 HTTP ERROR: Timeout was reached (28)\n",
      "1718481943 HTTP ERROR: Timeout was reached (28)\n",
      "1718482690 HTTP ERROR: Timeout was reached (28)\n",
      "24/06/15 22:41:43 WARN TaskSetManager: Stage 2 contains a task of very large size (6863 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/15 22:41:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/06/16 01:14:15 WARN TaskSetManager: Stage 3 contains a task of very large size (9452 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/16 01:14:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "1718493750 HTTP ERROR: Failure when receiving data from the peer (56)           \n",
      "1718494223 HTTP ERROR: Failure when receiving data from the peer (56)\n",
      "24/06/16 03:34:26 WARN TaskSetManager: Stage 4 contains a task of very large size (8038 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/16 03:34:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#########turn off all warnings\n",
    "import sys\n",
    "# Redirect stderr to /dev/null\n",
    "sys.stderr = open('/dev/null', 'w')\n",
    "\n",
    "\n",
    "########### turn on all warnings\n",
    "# import sys\n",
    "# # Store the original stderr stream\n",
    "# original_stderr = sys.stderr\n",
    "# # Redirect stderr to /dev/null\n",
    "# sys.stderr = open('/dev/null', 'w')\n",
    "# # Your code here...\n",
    "# # To revert back to the original stderr stream\n",
    "# sys.stderr = original_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for creating the database where the scraped ASN numbers and there info will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the database class \n",
    "class DBConnection:\n",
    "    def __init__(self, dbname = 'data.db'):\n",
    "        self.db_con = sqlite3.connect(dbname)\n",
    "        self.db_cur = self.db_con.cursor()\n",
    "    \n",
    "    def create_table(self):\n",
    "        self.db_cur.execute('''create table if not exists au_systems (\n",
    "            asn integer PRIMARY KEY,\n",
    "            organization text,\n",
    "            country text\n",
    "        )''')\n",
    "\n",
    "    def insert(self, asn, org_info, country):\n",
    "        self.db_cur.execute(f\"insert into au_systems values ('{asn}', '{org_info}', '{country}')\")\n",
    "\n",
    "    def insert(self, asn, org_info, country):\n",
    "        try:\n",
    "            self.db_cur.execute(\"INSERT OR IGNORE INTO au_systems VALUES (?, ?, ?)\", (asn, org_info, country))\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(\"Database locked. Retrying in 1 second...\")\n",
    "            time.sleep(1) \n",
    "            # Retry the insert operation\n",
    "            self.insert(asn, org_info, country)  \n",
    "\n",
    "    def find(self, column, value):\n",
    "        self.db_cur.execute(f\"select * from au_systems where {column}='{value}'\")\n",
    "        return self.db_cur.fetchall()\n",
    "\n",
    "    def find_one(self, column, value):\n",
    "        self.db_cur.execute(f\"select * from au_systems where {column}='{value}'\")\n",
    "        return self.db_cur.fetchone()\n",
    "    \n",
    "    def find_all(self):\n",
    "        self.db_cur.execute('select * from au_systems')\n",
    "        return self.db_cur.fetchall()\n",
    "    \n",
    "    def commit(self):\n",
    "        self.db_con.commit()\n",
    "\n",
    "    def close(self):\n",
    "        self.db_con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the database for ASNs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML Parser Class\n",
    "class HTMLFilter(HTMLParser):\n",
    "    text = \"\"\n",
    "    def handle_data(self, data):\n",
    "        self.text += data\n",
    "        \n",
    "html_filter = HTMLFilter()\n",
    "\n",
    "# Download content from the resource\n",
    "url = \"https://www.cidr-report.org/as2.0/autnums.html\"\n",
    "r = requests.get(url, allow_redirects = True)\n",
    "html_filter.feed(r.content.decode(\"utf-8\"))\n",
    "\n",
    "db = DBConnection()\n",
    "db.create_table()\n",
    "\n",
    "# Populate the database\n",
    "lst = html_filter.text.splitlines( )\n",
    "for i in range(14,len(lst)-8):\n",
    "    line = lst[i]\n",
    "    asn, org_info, country_code = int(line[2:8].strip()), line[8:-4].strip(), line[-2:]\n",
    "    db.insert(asn, org_info, country_code)\n",
    "\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First collect_update is slower version, transformation 2 includes more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Download BGP updates with stream\n",
    "def collect_updates(target_path_asns, start_time, end_time):\n",
    "    peer_as_list = []\n",
    "    path_list = []\n",
    "    collector_list = []\n",
    "    time_list = []\n",
    "    timer = 0\n",
    "\n",
    "    for tpa in target_path_asns:\n",
    "        tpa = [str(asn) for asn in tpa]\n",
    "        filter_str = \"path {0}\".format(\"|\".join(tpa))\n",
    "        stream = pybgpstream.BGPStream(\n",
    "            from_time=start_time,\n",
    "            until_time=end_time,\n",
    "            collectors=[],\n",
    "            record_type=\"updates\",\n",
    "            filter=filter_str\n",
    "        )\n",
    "\n",
    "        for record in stream:\n",
    "            elem = record.get_next_elem()\n",
    "            while elem is not None:\n",
    "                as_path_str = elem.fields.get(\"as-path\", \"\")\n",
    "                if as_path_str and as_path_str != \"{}\":\n",
    "                    as_path_str = as_path_str.replace('{', '').replace('}', '')\n",
    "                    as_path_parts = as_path_str.split()\n",
    "                    as_path = [int(part) for part in as_path_parts if part.isdigit()]\n",
    "                    if as_path:\n",
    "                        peer_as_list.append(elem.peer_asn)\n",
    "                        path_list.append(as_path)\n",
    "                        collector_list.append(record.collector)\n",
    "                        time_list.append(datetime.utcfromtimestamp(record.time).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "                elem = record.get_next_elem() \n",
    "        clear_output(wait=True)\n",
    "        timer += 1\n",
    "        print(100*(timer/len(target_path_asns)), '%')\n",
    "\n",
    "    df_update = spark.createDataFrame(zip(peer_as_list, path_list, collector_list, time_list),\n",
    "                                      schema=[\"Peer AS\", \"Path\", \"Router Collector Name\", \"Time\"])\n",
    "    return df_update\n",
    "\n",
    "# Divide the ASN of the target country in chunks so it fits the filter string of the stream\n",
    "def divide_list(lst, first_chunk_size=39, rest_chunk_size=34):\n",
    "    divided_list = []\n",
    "    # Create the first chunk\n",
    "    if lst:\n",
    "        first_chunk = lst[:first_chunk_size]\n",
    "        divided_list.append(first_chunk)\n",
    "        lst = lst[first_chunk_size:]\n",
    "    # Create the rest of the chunks\n",
    "    for i in range(0, len(lst), rest_chunk_size):\n",
    "        divided_list.append(lst[i:i + rest_chunk_size])\n",
    "    return divided_list\n",
    "\n",
    "# Collect all ASN belongin to the target country\n",
    "def collect_ASN(country_code, fraction_asns):\n",
    "    # Create a DBConnection instance\n",
    "    db = DBConnection()\n",
    "\n",
    "    # Find all ASNs that are Chinese\n",
    "    chinese_asns = db.find('country', country_code)\n",
    "\n",
    "    # Convert the result to a pandas DataFrame\n",
    "    df = pd.DataFrame(chinese_asns, columns=['ASN', 'Organization', 'Country'])\n",
    "    db.close()\n",
    "    \n",
    "    ASNs = df['ASN'].tolist()\n",
    "    x = round(len(ASNs)*fraction_asns)\n",
    "    return divide_list(ASNs[:x])\n",
    "\n",
    "# Collect the selected updates from the selected target country\n",
    "def collect_BGP_updates3(country_code, start_time, end_time, fraction_asns):\n",
    "    target = collect_ASN(country_code, fraction_asns)\n",
    "    BGPs = collect_updates(target, start_time, end_time)\n",
    "    return BGPs\n",
    "\n",
    "# Download and save the BGP updates of the first week of Jan for the selected years\n",
    "def download_data_and_save(start_year, end_year):\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        start_time = datetime(year, 1, 7).strftime(\"%Y-%m-%d 00:00:00 UTC\")\n",
    "        end_time = datetime(year, 1, 7).strftime(\"%Y-%m-%d 23:59:59 UTC\")\n",
    "        df_update = collect_BGP_updates3('KH', start_time, end_time, 1)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        file_name = f\"BGP_data_{year}_j7\"\n",
    "        df_update.write.format(\"parquet\").save(file_name)\n",
    "\n",
    "# Download and save the BGP updates of the selected days in Jan of the selected year\n",
    "def download_data_and_save2(start_day, end_day, year):\n",
    "\n",
    "    for day in range(start_day, end_day + 1):\n",
    "        start_time = datetime(year, 1, day).strftime(\"%Y-%m-%d 00:00:00 UTC\")\n",
    "        end_time = datetime(year, 1, day).strftime(\"%Y-%m-%d 23:59:59 UTC\")\n",
    "        df_update = collect_BGP_updates3('KH', start_time, end_time, 1)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        file_name = f\"BGP_data_{year}_feb_j{day}\"\n",
    "        df_update.write.format(\"parquet\").save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download selected days of January for selected year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BGP Data Downloader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "download_data_and_save2(start_day=1, end_day=7, year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download first week of January from selected years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"download\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "download_data_and_save(2016, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
